{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf514fb9",
   "metadata": {},
   "source": [
    "\n",
    "# Data Cleaning Notebook – Heritage Housing Prices\n",
    "\n",
    "## Objectives\n",
    "\n",
    "This notebook prepares the dataset for machine learning modeling by:\n",
    "\n",
    "- Identifying and evaluating missing values\n",
    "- Dropping sparse or non-predictive features\n",
    "- Splitting the data into training and testing sets\n",
    "- Saving cleaned datasets for modeling\n",
    "\n",
    "## Inputs\n",
    "\n",
    "- outputs\\datasets\\collection\\house_prices_records.csv\n",
    "\n",
    "## Outputs\n",
    "\n",
    "- Generate cleaned Train and Test sets, both saved under outputs/datasets/cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdbcc32",
   "metadata": {},
   "source": [
    "\n",
    "## Roadmap\n",
    "\n",
    "1. Load raw collected dataset  \n",
    "2. Identify missing values and assess feature completeness  \n",
    "3. Split into training and testing sets   \n",
    "4. Drop sparse or low-value features\n",
    "5. Save cleaned data  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed4aeb1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1457cc7c",
   "metadata": {},
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca3dacc",
   "metadata": {},
   "source": [
    "We need to change the working directory from its current folder to its parent folder\n",
    "* We access the current directory with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a581a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b414ab87",
   "metadata": {},
   "source": [
    "We want to make the parent of the current directory the new current directory\n",
    "* `os.path.dirname()` gets the parent directory\n",
    "* `os.chir()` defines the new current directory\n",
    "\n",
    "Then we confirm the new current directory by printing it with `os.getcwd()` again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab7f811",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"New working directory set to:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e49547",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea870d9",
   "metadata": {},
   "source": [
    "# Load Collected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a4b4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_raw_path = \"outputs/datasets/collection/house_prices_records.csv\"\n",
    "df = pd.read_csv(df_raw_path)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754f3ee4",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450c9d1c",
   "metadata": {},
   "source": [
    "In this section, we are interested in checking the distribution and shape of variables with missing data.\n",
    "\n",
    "So we list all variables with missing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585a9a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_with_missing_data = df.columns[df.isna().sum() > 0].to_list()\n",
    "vars_with_missing_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b6c053",
   "metadata": {},
   "source": [
    "Then we create a profile with the variables with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d139d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ydata_profiling import ProfileReport\n",
    "if vars_with_missing_data:\n",
    "    profile = ProfileReport(df=df[vars_with_missing_data], minimal=True)\n",
    "    profile.to_notebook_iframe()\n",
    "else:\n",
    "    print(\"There are no variables with missing data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c35d87c",
   "metadata": {},
   "source": [
    "# Correlation and PPS Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d441373",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ppscore as pps\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def heatmap_corr(df, threshold, figsize=(20, 12), font_annot=8):\n",
    "    if len(df.columns) > 1:\n",
    "        mask = np.zeros_like(df, dtype=bool)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "        mask[abs(df) < threshold] = True\n",
    "\n",
    "        fig, axes = plt.subplots(figsize=figsize)\n",
    "        sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
    "                    mask=mask, cmap='viridis', annot_kws={\"size\": font_annot}, ax=axes,\n",
    "                    linewidth=0.5\n",
    "                    )\n",
    "        axes.set_yticklabels(df.columns, rotation=0)\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def heatmap_pps(df, threshold, figsize=(20, 12), font_annot=8):\n",
    "    if len(df.columns) > 1:\n",
    "        mask = np.zeros_like(df, dtype=bool)\n",
    "        mask[abs(df) < threshold] = True\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax = sns.heatmap(df, annot=True, xticklabels=True, yticklabels=True,\n",
    "                         mask=mask, cmap='rocket_r', annot_kws={\"size\": font_annot},\n",
    "                         linewidth=0.05, linecolor='grey')\n",
    "        plt.ylim(len(df.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def CalculateCorrAndPPS(df):\n",
    "    df_corr_spearman = df.corr(method=\"spearman\", numeric_only=True)\n",
    "    df_corr_pearson = df.corr(method=\"pearson\", numeric_only=True)\n",
    "\n",
    "    import warnings\n",
    "\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=FutureWarning) # Ignore FutureWarning for ppscore to improve readability\n",
    "        pps_matrix_raw = pps.matrix(df)\n",
    "        pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')\n",
    "\n",
    "        pps_score_stats = pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore']).describe().T\n",
    "        print(\"\\nPPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
    "        print(pps_score_stats.round(3))\n",
    "\n",
    "    return df_corr_pearson, df_corr_spearman, pps_matrix\n",
    "\n",
    "\n",
    "def DisplayCorrAndPPS(df_corr_pearson, df_corr_spearman, pps_matrix, CorrThreshold, PPS_Threshold,\n",
    "                      figsize=(20, 12), font_annot=8):\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"* Analyse how the target variable for your ML models are correlated with other variables (features and target)\")\n",
    "    print(\"* Analyse multi-colinearity, that is, how the features are correlated among themselves\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Spearman Correlation ***\")\n",
    "    print(\"It evaluates monotonic relationship \\n\")\n",
    "    heatmap_corr(df=df_corr_spearman, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Pearson Correlation ***\")\n",
    "    print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
    "    heatmap_corr(df=df_corr_pearson, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
    "    print(f\"PPS detects linear or non-linear relationships between two columns.\\n\"\n",
    "          f\"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
    "    heatmap_pps(df=pps_matrix, threshold=PPS_Threshold, figsize=figsize, font_annot=font_annot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bedef5",
   "metadata": {},
   "source": [
    "Calculate Correlations and Power Predictive Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c3808",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corr_pearson, df_corr_spearman, pps_matrix = CalculateCorrAndPPS(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e78e5e",
   "metadata": {},
   "source": [
    "Display Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c43332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DisplayCorrAndPPS(df_corr_pearson = df_corr_pearson,\n",
    "                  df_corr_spearman = df_corr_spearman, \n",
    "                  pps_matrix = pps_matrix,\n",
    "                  CorrThreshold = 0.4, PPS_Threshold =0.2,\n",
    "                  figsize=(12,10), font_annot=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1ea51c",
   "metadata": {},
   "source": [
    "### Top Predictors of SalePrice\n",
    "\n",
    "All three methods consistently highlight the following variables as highly predictive of house prices:\n",
    "\n",
    "- `OverallQual` (Quality of materials/finish)\n",
    "\n",
    "- `GrLivArea` (Above-ground living area)\n",
    "\n",
    "- `GarageArea`\n",
    "\n",
    "- `TotalBsmtSF` (Basement area)\n",
    "\n",
    "- `YearBuilt`\n",
    "\n",
    "- `GarageYrBlt` (PPS identified this as particularly strong)\n",
    "\n",
    "These features seem like good candidates to retain for model development.\n",
    "\n",
    "### Multicollinearity Considerations\n",
    "\n",
    "Some variables are strongly correlated with each other, which may cause multicollinearity in linear models:\n",
    "\n",
    "- `GrLivArea` ↔ `1stFlrSF` (0.69)\n",
    "\n",
    "- `1stFlrSF` ↔ `TotalBsmtSF` (0.82)\n",
    "\n",
    "- `GarageArea` ↔ `GrLivArea` (0.57)\n",
    "\n",
    "These relationships may lead to redundancy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec1425",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38017405",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d38102",
   "metadata": {},
   "source": [
    "## Assessing Missing Data Levels\n",
    "\n",
    "- Custom function to display missing data levels in a DataFrame, it shows the absolute levels, relative levels and data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e155ec5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_missing_data(df):\n",
    "    missing_abs = df.isnull().sum()\n",
    "    missing_pct = round(missing_abs / len(df) * 100, 2)\n",
    "    df_missing_data = (pd.DataFrame(\n",
    "                            data={\"RowsWithMissingData\": missing_abs,\n",
    "                                   \"PercentageOfDataset\": missing_pct,\n",
    "                                   \"DataType\": df.dtypes}\n",
    "                                    )\n",
    "                          .sort_values(by=['PercentageOfDataset'], ascending=False)\n",
    "                          .query(\"PercentageOfDataset > 0\")\n",
    "                          )\n",
    "\n",
    "    return df_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba66352",
   "metadata": {},
   "source": [
    "Check missing data levels for the collected dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20003ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_missing_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c59a2d",
   "metadata": {},
   "source": [
    "## Dealing with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0174120",
   "metadata": {},
   "source": [
    "### Split Train and Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240fc549",
   "metadata": {},
   "source": [
    "We split the dataset before cleaning to avoid data leakage.\n",
    "\n",
    "This ensures that:\n",
    "\n",
    "- All cleaning decisions (like which variables to drop) are based solely on the training data\n",
    "\n",
    "- The test set remains a realistic “unseen” sample to evaluate model performance\n",
    "\n",
    "- We simulate what would happen in a real-world deployment, where new data is cleaned using a process built on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b071566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TrainSet, TestSet = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"TrainSet shape: {TrainSet.shape}\")\n",
    "print(f\"TestSet shape: {TestSet.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb30474",
   "metadata": {},
   "source": [
    "###  Re-Evaluate Missing Data in Train Set\n",
    "Now we check missing data only in the training set, which we will use to guide cleaning decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16582785",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_missing_data = evaluate_missing_data(TrainSet)\n",
    "print(f\"* There are {df_missing_data.shape[0]} variables with missing data \\n\")\n",
    "df_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e2c9b5",
   "metadata": {},
   "source": [
    "### Data Cleaning Decision: Dropping Sparse Features\n",
    "\n",
    "Based on the profiling report, we reviewed each variable with missing values and made decisions grounded in their:\n",
    "\n",
    "- Missing percentage\n",
    "- Predictive value potential\n",
    "- Domain relevance\n",
    "\n",
    "#### Features to Drop:\n",
    "\n",
    "- **`EnclosedPorch`** – 90.7% missing  \n",
    "  Too sparse to be useful. Even if imputed, it would contribute noise rather than signal.\n",
    "\n",
    "- **`WoodDeckSF`** – 89.4% missing  \n",
    "  Very low coverage and low variability among non-missing values. Similar to `EnclosedPorch`, better removed.\n",
    "\n",
    "These features are dropped to simplify the dataset and avoid bias or overfitting due to poor-quality data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b31e55",
   "metadata": {},
   "source": [
    "### Test Dropping Variables on the Training Set\n",
    "\n",
    "We start by applying the drop transformation **only to the training set** and saving the result to a temporary DataFrame. This lets us assess the effect before committing to the change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbfe09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import DropFeatures\n",
    "\n",
    "variables_to_drop = ['EnclosedPorch', 'WoodDeckSF']\n",
    "\n",
    "dropper = DropFeatures(features_to_drop=variables_to_drop)\n",
    "dropper.fit(TrainSet)\n",
    "\n",
    "# Preview effect of dropping columns\n",
    "TrainSet_preview = dropper.transform(TrainSet)\n",
    "TrainSet_preview.head(5)\n",
    "TrainSet_preview.shape, TrainSet.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a4cbc6",
   "metadata": {},
   "source": [
    "### Assess the Effect of Dropping Columns\n",
    "\n",
    "We're removing columns, not rows — so the number of samples remains the same.  \n",
    "But we want to confirm how many columns are being dropped and whether they were meaningful.\n",
    "\n",
    "In this case, `EnclosedPorch` and `WoodDeckSF` were sparse and mostly zero, with little to no predictive power based on prior correlation and EDA analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a837a714",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Before drop: {TrainSet.shape[1]} columns\")\n",
    "print(f\"After drop: {TrainSet_preview.shape[1]} columns\")\n",
    "print(f\"Dropped columns: {variables_to_drop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1e5ded",
   "metadata": {},
   "source": [
    "### Apply the Transformation to Train and Test Sets\n",
    "\n",
    "Now that we’re satisfied, we apply the same dropper to both sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407bcc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropper = DropFeatures(features_to_drop=variables_to_drop)\n",
    "dropper.fit(TrainSet)\n",
    "\n",
    "TrainSet = dropper.transform(TrainSet)\n",
    "TestSet = dropper.transform(TestSet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d574e811",
   "metadata": {},
   "source": [
    "### Re-Evaluate Missing Data\n",
    "\n",
    "We check for remaining missing values after removing sparse features.  \n",
    "Any remaining columns with missing data will be handled in the modeling notebook during imputation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1e5d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_missing_data(TrainSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f29a17",
   "metadata": {},
   "source": [
    "# Save Cleaned Train and Test Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c61123",
   "metadata": {},
   "source": [
    "Here we create create outputs/datasets/collection folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bb7720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    os.makedirs(\"outputs/datasets/cleaned\", exist_ok=True)\n",
    "    TrainSet.to_csv(\"outputs/datasets/cleaned/TrainSetCleaned.csv\", index=False)\n",
    "    TestSet.to_csv(\"outputs/datasets/cleaned/TestSetCleaned.csv\", index=False)\n",
    "except Exception as e:\n",
    "    print(f\"Error creating directories or saving files: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "990ab4a5",
   "metadata": {},
   "source": [
    "# Push cleaned data to Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1390965c",
   "metadata": {},
   "source": [
    "You can now push the changes to your GitHub repository, using the Git commands (git add, git commit, git push)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aac488",
   "metadata": {},
   "source": [
    "## Conclusions and Next Steps\n",
    "\n",
    "- Identified variables with missing data and evaluated their type and proportion.\n",
    "- Dropped `EnclosedPorch` and `WoodDeckSF` due to sparsity and low predictive potential.\n",
    "- Split dataset into training and testing subsets before applying any modeling logic.\n",
    "- Saved cleaned datasets for reuse in the upcoming modeling notebook.\n",
    "\n",
    "In the next notebook, we’ll:\n",
    "- Impute remaining missing values\n",
    "- Encode categorical features\n",
    "- Perform feature scaling and model training\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
