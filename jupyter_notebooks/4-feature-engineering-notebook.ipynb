{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Feature Engineering Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Engineer features for Regression\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* inputs/datasets/cleaned/TrainSet.csv\n",
        "* inputs/datasets/cleaned/TestSet.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* generate a list with variables to engineer\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook covers the full feature engineering process applied to the cleaned housing dataset. Feature engineering is a critical step in preparing the data for machine learning, as it transforms raw variables into a format that improves the model’s ability to learn patterns. \n",
        "\n",
        "We will address:\n",
        "- Encoding of categorical variables\n",
        "- Normalization of skewed numerical features\n",
        "- Management of multicollinearity\n",
        "- Evaluation of transformation effects via visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with `os.getcwd()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* `os.path.dirname()` gets the parent directory\n",
        "* `os.chir()` defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "print(\"New working directory set to:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Cleaned Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_set_path = \"outputs/datasets/cleaned/TrainSetCleaned.csv\"\n",
        "TrainSet = pd.read_csv(train_set_path)\n",
        "TrainSet.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_set_path = 'outputs/datasets/cleaned/TestSetCleaned.csv'\n",
        "TestSet = pd.read_csv(test_set_path)\n",
        "TestSet.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "pandas_report = ProfileReport(df=TrainSet, minimal=True)\n",
        "pandas_report.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Impute Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before any modeling or transformation can occur, it's essential to address missing data. This ensures model compatibility and avoids distortions during encoding and scaling. \n",
        "\n",
        "We begin by identifying variables with missing values and apply suitable imputation strategies based on the variable type and domain knowledge. Where relevant, we also add binary indicators to flag imputed values for potential predictive insight.\n",
        "We apply imputation strategies based on data type and domain understanding.\n",
        "\n",
        "We see that there are 64 instances of 'Zero' in `GarageArea` and 64 empty instances in `GarageYrBlt`, leading us to infer that the missing data in `GarageYrBlt` relates to the lack of a garage in that house.\n",
        "\n",
        "We start by handling missing values before encoding or scaling. This includes:\n",
        "\n",
        "- Creating binary flags for missing values (e.g. `GarageYrBlt_missing`)\n",
        "- Using grouped median imputation for `LotFrontage` based on `Neighborhood`\n",
        "- Using constant values (like `0` or `\"None\"`) for features where missingness signals absence\n",
        "- Applying SimpleImputer to other features using strategies based on metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add missingness indicators\n",
        "TrainSet['GarageYrBlt_missing'] = TrainSet['GarageYrBlt'].isna().astype(int)\n",
        "TrainSet['LotFrontage_missing'] = TrainSet['LotFrontage'].isna().astype(int)\n",
        "\n",
        "TestSet['GarageYrBlt_missing'] = TestSet['GarageYrBlt'].isna().astype(int)\n",
        "TestSet['LotFrontage_missing'] = TestSet['LotFrontage'].isna().astype(int)\n",
        "\n",
        "# Use global median for LotFrontage\n",
        "lotfrontage_median = TrainSet['LotFrontage'].median()\n",
        "TrainSet['LotFrontage'] = TrainSet['LotFrontage'].fillna(lotfrontage_median)\n",
        "TestSet['LotFrontage'] = TestSet['LotFrontage'].fillna(lotfrontage_median)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imputation Plan\n",
        "\n",
        "|Feature | Metadata Insight | Strategy  | Notes|\n",
        "|--------|------------------|----------------------------|------|\n",
        "|`LotFrontage`  | Linear feet of street connected to property | Median | Group median |\n",
        "|`GarageYrBlt`  | Year garage was built (1900–2010); missing if no garage | Fill with `0` | 0 clearly means no garage (can also flag with new feature if needed)|\n",
        "|`2ndFlrSF`     | Square footage of second floor (0–2065); 0 is common | Fill with `0` | No imputation needed — zero is valid|\n",
        "|`MasVnrArea`   | Masonry veneer area (0–1600); 0 means no veneer | Fill with `0` | 0 is semantically meaningful|\n",
        "|`BedroomAbvGr` | Bedrooms above ground; 0–8 range | Median | Could also test for mode; median is fine|\n",
        "|`BsmtExposure` | Exposure rating or `\"None\"` for no basement | Fill with `\"None\"` | Use `\"None\"` instead of \"Missing\" to match domain encoding|\n",
        "|`BsmtFinType1` | Finish type or `\"None\"` if no basement | Fill with `\"None\"` | `\"None\"` is an actual category in metadata|\n",
        "|`GarageFinish` | Garage interior finish or `\"None\"` if no garage | Fill with `\"None\"` | Use `\"None\"` for clarity and alignment with domain semantics|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# LotFrontage is already handled\n",
        "numerical_impute_zero = ['2ndFlrSF', 'MasVnrArea', 'GarageYrBlt']\n",
        "numerical_impute_median = ['BedroomAbvGr']\n",
        "categorical_fill_none = ['BsmtExposure', 'BsmtFinType1', 'GarageFinish']\n",
        "\n",
        "zero_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
        "median_imputer = SimpleImputer(strategy='median')\n",
        "cat_imputer = SimpleImputer(strategy='constant', fill_value='None')\n",
        "\n",
        "imputer_transformer = ColumnTransformer(transformers=[\n",
        "    ('num_zero', zero_imputer, numerical_impute_zero),\n",
        "    ('num_median', median_imputer, numerical_impute_median),\n",
        "    ('cat_fill', cat_imputer, categorical_fill_none)\n",
        "], remainder='passthrough')\n",
        "\n",
        "TrainSet_imputed = pd.DataFrame(\n",
        "    imputer_transformer.fit_transform(TrainSet),\n",
        "    columns=numerical_impute_zero + numerical_impute_median + categorical_fill_none +\n",
        "            [col for col in TrainSet.columns if col not in numerical_impute_zero + numerical_impute_median + categorical_fill_none]\n",
        ")\n",
        "\n",
        "TestSet_imputed = pd.DataFrame(\n",
        "    imputer_transformer.transform(TestSet),\n",
        "    columns=TrainSet_imputed.columns\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preview Results After Imputation Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet_imputed.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check for remaining missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Remaining missing values in TrainSet:\")\n",
        "print(TrainSet_imputed.isnull().sum().sort_values(ascending=False).head())\n",
        "print(\"Remaining missing values in TestSet:\")\n",
        "print(TestSet_imputed.isnull().sum().sort_values(ascending=False).head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confirm new flags exist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nColumns added as missingness flags:\")\n",
        "print([col for col in TrainSet_imputed.columns if '_missing' in col])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confirm data types and number of columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nData types after transformation:\")\n",
        "print(TrainSet_imputed.dtypes.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post-Imputation Correlation & PPS Check\n",
        "\n",
        "Now that we've imputed missing values and added flags, we reassess the feature relationships.\n",
        "\n",
        "This helps us:\n",
        "- Re-confirm top predictors of `SalePrice`\n",
        "- Detect any new multicollinearity\n",
        "- Spot newly valuable features (e.g. missingness indicators)\n",
        "\n",
        "We'll examine both:\n",
        "- **Pearson/Spearman correlation** (for linear and monotonic relationships)\n",
        "- **Power Predictive Score (PPS)** (for general predictive strength)\n",
        "NOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Preview column data types\n",
        "print(\" Column data types after imputation:\")\n",
        "print(TrainSet_imputed.dtypes.value_counts())\n",
        "\n",
        "# 2. Get object columns for review\n",
        "object_cols = TrainSet_imputed.select_dtypes(include='object').columns.tolist()\n",
        "print(f\"\\n Object-type columns to review ({len(object_cols)}):\")\n",
        "print(object_cols)\n",
        "\n",
        "# 3. Preview unique values in a few columns\n",
        "print(\"\\n Sample values from first few object columns:\")\n",
        "for col in object_cols[:5]:\n",
        "    print(f\"- {col}: {TrainSet_imputed[col].unique()[:5]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoding Categorical Features\n",
        "\n",
        "With missing data handled, we turn to encoding. Machine learning algorithms require input features to be numeric, so we must convert all categorical features into numerical representations. In this project, we use **ordinal encoding**, which preserves category order where meaningful and allows the use of a single numeric column per variable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Almost everything is still stored as object, including numeric-looking values like:\n",
        "\n",
        "- `'2ndFlrSF'`: [0.0, 772.0, …]\n",
        "\n",
        "- `'BedroomAbvGr'`: [3.0, 2.0, …]\n",
        "\n",
        "    These need to be converted to numeric types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Truly categorical variables (with labels) include:\n",
        "\n",
        "- `'BsmtExposure'`: ['No', 'Av', 'Gd', 'Mn', 'None']\n",
        "\n",
        "- `'BsmtFinType1'`, `'GarageFinish'`, `'KitchenQual'`\n",
        "\n",
        "    These need encoding (Ordinal or One-Hot depending on model preference — we’ll use Ordinal for now to keep things simple for correlation and PPS)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. SalePrice is not yet in this list — but should be numeric and included. So we double-check this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"SalePrice type:\", TrainSet['SalePrice'].dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Convert numeric-looking object columns to numbers. So first we identify which object columns can be converted to numeric or leave it as object if conversion fails."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "for col in TrainSet_imputed.columns:\n",
        "    if TrainSet_imputed[col].dtype == 'object':\n",
        "        try:\n",
        "            TrainSet_imputed[col] = pd.to_numeric(TrainSet_imputed[col])\n",
        "            TestSet_imputed[col] = pd.to_numeric(TestSet_imputed[col])\n",
        "        except:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Encode remaining categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "# Manually specify true categorical features\n",
        "categorical_features = ['BsmtExposure', 'BsmtFinType1', 'GarageFinish', 'KitchenQual']\n",
        "\n",
        "# Fit and apply ordinal encoder\n",
        "encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
        "TrainSet_imputed[categorical_features] = encoder.fit_transform(TrainSet_imputed[categorical_features])\n",
        "TestSet_imputed[categorical_features] = encoder.transform(TestSet_imputed[categorical_features])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"🔄 Updated data types after conversion + encoding:\")\n",
        "print(TrainSet_imputed.dtypes.value_counts())\n",
        "\n",
        "# Confirm SalePrice is numeric\n",
        "print(\"SalePrice dtype:\", TrainSet['SalePrice'].dtype)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate Distribution Transformations\n",
        "\n",
        "Now that all features are numeric and encoded, we assess whether **distribution transformations** (e.g. log, Yeo-Johnson) could help normalize features and benefit certain algorithms (like linear models or KNN).\n",
        "\n",
        "We'll use a custom utility `FeatureEngineeringAnalysis` to preview the effect of various transformations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from feature_engine import transformation as vt\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "sns.set(style=\"whitegrid\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# %matplotlib inline\n",
        "\n",
        "\n",
        "def FeatureEngineeringAnalysis(df, analysis_type=None):\n",
        "    \"\"\"\n",
        "    - used for quick feature engineering on numerical and categorical variables\n",
        "    to decide which transformation can better transform the distribution shape\n",
        "    - Once transformed, use a reporting tool, like ydata-profiling, to evaluate distributions\n",
        "    \"\"\"\n",
        "    check_missing_values(df)\n",
        "    allowed_types = ['numerical', 'ordinal_encoder', 'outlier_winsorizer']\n",
        "    check_user_entry_on_analysis_type(analysis_type, allowed_types)\n",
        "    list_column_transformers = define_list_column_transformers(analysis_type)\n",
        "\n",
        "    # Loop in each variable and engineer the data according to the analysis type\n",
        "    df_feat_eng = pd.DataFrame([])\n",
        "    for column in df.columns:\n",
        "        # create additional columns (column_method) to apply the methods\n",
        "        df_feat_eng = pd.concat([df_feat_eng, df[column]], axis=1)\n",
        "        for method in list_column_transformers:\n",
        "            df_feat_eng[f\"{column}_{method}\"] = df[column]\n",
        "\n",
        "        # Apply transformers in respective column_transformers\n",
        "        df_feat_eng, list_applied_transformers = apply_transformers(\n",
        "            analysis_type, df_feat_eng, column)\n",
        "\n",
        "        # For each variable, assess how the transformations perform\n",
        "        transformer_evaluation(\n",
        "            column, list_applied_transformers, analysis_type, df_feat_eng)\n",
        "\n",
        "    return df_feat_eng\n",
        "\n",
        "\n",
        "def check_user_entry_on_analysis_type(analysis_type, allowed_types):\n",
        "    \"\"\" Check analysis type \"\"\"\n",
        "    if analysis_type is None:\n",
        "        raise SystemExit(\n",
        "            f\"You should pass analysis_type parameter as one of the following options: {allowed_types}\")\n",
        "    if analysis_type not in allowed_types:\n",
        "        raise SystemExit(\n",
        "            f\"analysis_type argument should be one of these options: {allowed_types}\")\n",
        "\n",
        "\n",
        "def check_missing_values(df):\n",
        "    if df.isna().sum().sum() != 0:\n",
        "        raise SystemExit(\n",
        "            f\"There is a missing value in your dataset. Please handle that before getting into feature engineering.\")\n",
        "\n",
        "\n",
        "def define_list_column_transformers(analysis_type):\n",
        "    \"\"\" Set suffix columns according to analysis_type\"\"\"\n",
        "    if analysis_type == 'numerical':\n",
        "        list_column_transformers = [\n",
        "            \"log_e\", \"log_10\", \"reciprocal\", \"power\", \"box_cox\", \"yeo_johnson\"]\n",
        "\n",
        "    elif analysis_type == 'ordinal_encoder':\n",
        "        list_column_transformers = [\"ordinal_encoder\"]\n",
        "\n",
        "    elif analysis_type == 'outlier_winsorizer':\n",
        "        list_column_transformers = ['iqr']\n",
        "\n",
        "    return list_column_transformers\n",
        "\n",
        "\n",
        "def apply_transformers(analysis_type, df_feat_eng, column):\n",
        "    for col in df_feat_eng.select_dtypes(include='category').columns:\n",
        "        df_feat_eng[col] = df_feat_eng[col].astype('object')\n",
        "\n",
        "    if analysis_type == 'numerical':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_Numerical(\n",
        "            df_feat_eng, column)\n",
        "\n",
        "    elif analysis_type == 'outlier_winsorizer':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_OutlierWinsorizer(\n",
        "            df_feat_eng, column)\n",
        "\n",
        "    elif analysis_type == 'ordinal_encoder':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_CategoricalEncoder(\n",
        "            df_feat_eng, column)\n",
        "\n",
        "    return df_feat_eng, list_applied_transformers\n",
        "\n",
        "\n",
        "def transformer_evaluation(column, list_applied_transformers, analysis_type, df_feat_eng):\n",
        "    # For each variable, assess how the transformations perform\n",
        "    print(f\"* Variable Analyzed: {column}\")\n",
        "    print(f\"* Applied transformation: {list_applied_transformers} \\n\")\n",
        "    for col in [column] + list_applied_transformers:\n",
        "\n",
        "        if analysis_type != 'ordinal_encoder':\n",
        "            DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "        else:\n",
        "            if col == column:\n",
        "                DiagnosticPlots_Categories(df_feat_eng, col)\n",
        "            else:\n",
        "                DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Categories(df_feat_eng, col):\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    sns.countplot(data=df_feat_eng, x=col, palette=[\n",
        "                  '#432371'], order=df_feat_eng[col].value_counts().index)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.suptitle(f\"{col}\", fontsize=30, y=1.05)\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Numerical(df, variable):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    sns.histplot(data=df, x=variable, kde=True, element=\"step\", ax=axes[0])\n",
        "    stats.probplot(df[variable], dist=\"norm\", plot=axes[1])\n",
        "    sns.boxplot(x=df[variable], ax=axes[2])\n",
        "\n",
        "    axes[0].set_title('Histogram')\n",
        "    axes[1].set_title('QQ Plot')\n",
        "    axes[2].set_title('Boxplot')\n",
        "    fig.suptitle(f\"{variable}\", fontsize=30, y=1.05)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def FeatEngineering_CategoricalEncoder(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "    try:\n",
        "        encoder = OrdinalEncoder(encoding_method='arbitrary', variables=[\n",
        "                                 f\"{column}_ordinal_encoder\"])\n",
        "        df_feat_eng = encoder.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_ordinal_encoder\")\n",
        "\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_ordinal_encoder\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked\n",
        "\n",
        "\n",
        "def FeatEngineering_OutlierWinsorizer(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "\n",
        "    # Winsorizer iqr\n",
        "    try:\n",
        "        disc = Winsorizer(\n",
        "            capping_method='iqr', tail='both', fold=1.5, variables=[f\"{column}_iqr\"])\n",
        "        df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_iqr\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_iqr\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked\n",
        "\n",
        "\n",
        "def FeatEngineering_Numerical(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "\n",
        "    # LogTransformer base e\n",
        "    try:\n",
        "        lt = vt.LogTransformer(variables=[f\"{column}_log_e\"])\n",
        "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_log_e\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_log_e\"], axis=1, inplace=True)\n",
        "\n",
        "    # LogTransformer base 10\n",
        "    try:\n",
        "        lt = vt.LogTransformer(variables=[f\"{column}_log_10\"], base='10')\n",
        "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_log_10\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_log_10\"], axis=1, inplace=True)\n",
        "\n",
        "    # ReciprocalTransformer\n",
        "    try:\n",
        "        rt = vt.ReciprocalTransformer(variables=[f\"{column}_reciprocal\"])\n",
        "        df_feat_eng = rt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_reciprocal\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_reciprocal\"], axis=1, inplace=True)\n",
        "\n",
        "    # PowerTransformer\n",
        "    try:\n",
        "        pt = vt.PowerTransformer(variables=[f\"{column}_power\"])\n",
        "        df_feat_eng = pt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_power\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_power\"], axis=1, inplace=True)\n",
        "\n",
        "    # BoxCoxTransformer\n",
        "    try:\n",
        "        bct = vt.BoxCoxTransformer(variables=[f\"{column}_box_cox\"])\n",
        "        df_feat_eng = bct.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_box_cox\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_box_cox\"], axis=1, inplace=True)\n",
        "\n",
        "    # YeoJohnsonTransformer\n",
        "    try:\n",
        "        yjt = vt.YeoJohnsonTransformer(variables=[f\"{column}_yeo_johnson\"])\n",
        "        df_feat_eng = yjt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_yeo_johnson\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_yeo_johnson\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Select Features to Analyze\n",
        "You should prioritize:\n",
        "\n",
        "A. Features highly correlated with SalePrice\n",
        "From your correlation analysis, these are likely candidates:\n",
        "\n",
        "|  Variable   |  Why Analyze?                                               |\n",
        "|-------------|-------------------------------------------------------------|\n",
        "| GrLivArea   | Often skewed right, strong correlation                      |\n",
        "| GarageArea  | May benefit from log transformation                         |\n",
        "| TotalBsmtSF | Can vary widely and often right-skewed                      |\n",
        "|  1stFlrSF   | Similar to above                                            |\n",
        "| OverallQual | Ordinal, might not need transformation but worth visualizing|\n",
        "\n",
        "B. New or imputed variables\n",
        "\n",
        "|Variable    | Why Analyze?                            |\n",
        "|------------|-----------------------------------------|\n",
        "|LotFrontage | We imputed it — so let's check its shape|\n",
        "|MasVnrArea  | Many zeroes — investigate transformation|\n",
        "|YearBuilt   | Time-based but numeric                  |\n",
        "|OpenPorchSF | Could have long tail                    |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Create Subsets to pass into the function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_numerical_features = [\n",
        "    'GrLivArea',\n",
        "    'GarageArea',\n",
        "    'TotalBsmtSF',\n",
        "    '1stFlrSF',\n",
        "    'LotFrontage',\n",
        "    'MasVnrArea',\n",
        "    'OpenPorchSF',\n",
        "    'YearBuilt'\n",
        "]\n",
        "\n",
        "# Run the tool on a few at a time (to avoid overload)\n",
        "FeatureEngineeringAnalysis(df=TrainSet_imputed[['MasVnrArea']], analysis_type='numerical')\n",
        "FeatureEngineeringAnalysis(df=TrainSet_imputed[['OpenPorchSF']], analysis_type='numerical')\n",
        "FeatureEngineeringAnalysis(df=TrainSet_imputed[['YearBuilt']], analysis_type='numerical')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How We Chose Which Transformations to Apply\n",
        "\n",
        "Each numerical variable was evaluated across multiple transformation methods using three visual diagnostics:\n",
        "- **Histogram**: checked for symmetry and bell-shaped distribution\n",
        "- **QQ Plot**: checked alignment with the diagonal (indicating normality)\n",
        "- **Boxplot**: checked for outlier compression and spread\n",
        "\n",
        "We selected the transformation that yielded the best visual improvement while preserving interpretability. Below is a summary of the selected transformations:\n",
        "\n",
        "| Variable        | Transformation Applied | Reasoning                                                                 |\n",
        "|-----------------|------------------------|---------------------------------------------------------------------------|\n",
        "| `GrLivArea`     | `log_e`                | Reduced right-skew and improved normality visually                        |\n",
        "| `GarageArea`    | `yeo_johnson`          | Handles zero values, improved symmetry                                   |\n",
        "| `1stFlrSF`      | `log_10`               | Significantly improved QQ plot and histogram                             |\n",
        "| `TotalBsmtSF`   | `power`                | Strong visual improvement in distribution shape                          |\n",
        "| `LotFrontage`   | `yeo_johnson`          | Smoothed outliers and improved bell-shaped symmetry                      |\n",
        "| `OverallQual`   | None                   | Ordinal variable, distribution already clean and interpretable           |\n",
        "| `YearBuilt`     | None                   | Discrete time-based variable; transformations distorted interpretability |\n",
        "\n",
        "Transformations not selected (like `reciprocal` or `box_cox`) were either less interpretable or introduced new artifacts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SmartCorrelatedSelection Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To improve model stability and interpretability, we aim to reduce multicollinearity — the presence of strong correlations between independent variables. We use domain knowledge, Spearman correlation analysis, and variance-based filtering to remove redundant features when necessary.\n",
        "\n",
        "* Step 1: Select variable(s)\n",
        "    - for this transformer, we don't need to select variables, since we need all variables for this transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 2: Create a separate DataFrame, with your variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = TrainSet_imputed.copy()\n",
        "df_engineering.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 3: Create engineered variables(s) applying the transformation(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "corr_sel = SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.8, selection_method=\"variance\")\n",
        "\n",
        "corr_sel.fit_transform(df_engineering)\n",
        "corr_sel.correlated_feature_sets_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_sel.features_to_drop_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusions and Next Steps\n",
        "\n",
        "### Key Outcomes\n",
        "- All missing values were imputed using context-appropriate strategies.\n",
        "- Categorical variables were encoded using ordinal encoding to preserve interpretability.\n",
        "- Several numerical features were transformed using techniques like `log`, `box-cox`, and `yeo-johnson` to reduce skewness and improve normality.\n",
        "- Visual diagnostics (histogram, QQ plot, boxplot) guided the selection of transformations.\n",
        "- Redundant features and those inappropriate for modeling (e.g. highly correlated or low variance) were dropped where applicable.\n",
        "\n",
        "### Next Steps\n",
        "- Finalize the modeling dataset by consolidating transformed features and dropping unused variants.\n",
        "- Conduct feature scaling if required (depending on the chosen algorithm).\n",
        "- Train baseline models and compare performance (e.g., Linear Regression, Random Forest).\n",
        "- Apply feature importance analysis post-modeling to validate choices made during feature engineering."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
