{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Feature Engineering Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Engineer features for Regression\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* inputs/datasets/cleaned/TrainSet.csv\n",
        "* inputs/datasets/cleaned/TestSet.csv\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* outputs\\datasets\\engineered\\TrainSet_Engineered.csv\n",
        "* outputs\\datasets\\engineered\\TestSet_Engineered.csv\n",
        "* generate a list with variables to engineer\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook covers the full feature engineering process applied to the cleaned housing dataset. Feature engineering is a critical step in preparing the data for machine learning, as it transforms raw variables into a format that improves the model’s ability to learn patterns. \n",
        "\n",
        "We will address:\n",
        "- Enconding of categorical features\n",
        "- Normalization of skewed numerical features\n",
        "- Evaluation of transformation effects via visualization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "# Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* We are assuming you will store the notebooks in a subfolder, therefore when running the notebook in the editor, you will need to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder\n",
        "* We access the current directory with `os.getcwd()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* `os.path.dirname()` gets the parent directory\n",
        "* `os.chir()` defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [],
      "source": [
        "print(\"New working directory set to:\", os.getcwd())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "# Load Cleaned Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_set_path = \"outputs/datasets/cleaned/TrainSetCleaned.csv\"\n",
        "TrainSet = pd.read_csv(train_set_path)\n",
        "TrainSet.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_set_path = 'outputs/datasets/cleaned/TestSetCleaned.csv'\n",
        "TestSet = pd.read_csv(test_set_path)\n",
        "TestSet.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "pandas_report = ProfileReport(df=TrainSet, minimal=True)\n",
        "pandas_report.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "# Impute Missing Values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before any modeling or transformation can occur, it's essential to address missing data. This ensures model compatibility and avoids distortions during encoding and scaling. \n",
        "\n",
        "We begin by identifying variables with missing values and apply suitable imputation strategies based on the variable type and domain knowledge. Where relevant, we also add binary indicators to flag imputed values for potential predictive insight.\n",
        "We apply imputation strategies based on data type and domain understanding.\n",
        "\n",
        "We see that there are 64 instances of 'Zero' in `GarageArea` and 64 empty instances in `GarageYrBlt`, leading us to infer that the missing data in `GarageYrBlt` relates to the lack of a garage in that house.\n",
        "\n",
        "We start by handling missing values before encoding or scaling. This includes:\n",
        "\n",
        "- Creating binary flags for missing values (e.g. `GarageYrBlt_missing`)\n",
        "- Using grouped median imputation for `LotFrontage` based on `Neighborhood`\n",
        "- Using constant values (like `0` or `\"None\"`) for features where missingness signals absence\n",
        "- Applying SimpleImputer to other features using strategies based on metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add missingness indicators\n",
        "TrainSet['GarageYrBlt_missing'] = TrainSet['GarageYrBlt'].isna().astype(int)\n",
        "TrainSet['LotFrontage_missing'] = TrainSet['LotFrontage'].isna().astype(int)\n",
        "\n",
        "TestSet['GarageYrBlt_missing'] = TestSet['GarageYrBlt'].isna().astype(int)\n",
        "TestSet['LotFrontage_missing'] = TestSet['LotFrontage'].isna().astype(int)\n",
        "\n",
        "# Use global median for LotFrontage\n",
        "lotfrontage_median = TrainSet['LotFrontage'].median()\n",
        "TrainSet['LotFrontage'] = TrainSet['LotFrontage'].fillna(lotfrontage_median)\n",
        "TestSet['LotFrontage'] = TestSet['LotFrontage'].fillna(lotfrontage_median)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imputation Plan\n",
        "\n",
        "|Feature | Metadata Insight | Strategy  | Notes|\n",
        "|--------|------------------|----------------------------|------|\n",
        "|`LotFrontage`  | Linear feet of street connected to property | Median | Group median |\n",
        "|`GarageYrBlt`  | Year garage was built (1900–2010); missing if no garage | Fill with `0` | 0 clearly means no garage (can also flag with new feature if needed)|\n",
        "|`2ndFlrSF`     | Square footage of second floor (0–2065); 0 is common | Fill with `0` | No imputation needed — zero is valid|\n",
        "|`MasVnrArea`   | Masonry veneer area (0–1600); 0 means no veneer | Fill with `0` | 0 is semantically meaningful|\n",
        "|`BedroomAbvGr` | Bedrooms above ground; 0–8 range | Median | Could also test for mode; median is fine|\n",
        "|`BsmtExposure` | Exposure rating or `\"None\"` for no basement | Fill with `\"None\"` | Use `\"None\"` instead of \"Missing\" to match domain encoding|\n",
        "|`BsmtFinType1` | Finish type or `\"None\"` if no basement | Fill with `\"None\"` | `\"None\"` is an actual category in metadata|\n",
        "|`GarageFinish` | Garage interior finish or `\"None\"` if no garage | Fill with `\"None\"` | Use `\"None\"` for clarity and alignment with domain semantics|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Columns to impute\n",
        "numerical_impute_zero = ['2ndFlrSF', 'MasVnrArea', 'GarageYrBlt']\n",
        "numerical_impute_median = ['BedroomAbvGr']\n",
        "categorical_fill_none = ['BsmtExposure', 'BsmtFinType1', 'GarageFinish']\n",
        "\n",
        "# Combine for full reference\n",
        "all_imputed_cols = numerical_impute_zero + numerical_impute_median + categorical_fill_none\n",
        "passthrough_cols = [col for col in TrainSet.columns if col not in all_imputed_cols]\n",
        "\n",
        "# Create imputers\n",
        "zero_imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
        "median_imputer = SimpleImputer(strategy='median')\n",
        "cat_imputer = SimpleImputer(strategy='constant', fill_value='None')\n",
        "\n",
        "# Column transformer\n",
        "imputer_transformer = ColumnTransformer(transformers=[\n",
        "    ('num_zero', zero_imputer, numerical_impute_zero),\n",
        "    ('num_median', median_imputer, numerical_impute_median),\n",
        "    ('cat_fill', cat_imputer, categorical_fill_none)\n",
        "], remainder='passthrough')\n",
        "\n",
        "# Fit and transform\n",
        "TrainSet_array = imputer_transformer.fit_transform(TrainSet)\n",
        "TestSet_array = imputer_transformer.transform(TestSet)\n",
        "\n",
        "# Combine column names\n",
        "final_columns = all_imputed_cols + passthrough_cols\n",
        "\n",
        "# Create DataFrames\n",
        "TrainSet_imputed = pd.DataFrame(TrainSet_array, columns=final_columns)\n",
        "TestSet_imputed = pd.DataFrame(TestSet_array, columns=final_columns)\n",
        "\n",
        "# Restore correct dtypes for imputed columns\n",
        "for col in numerical_impute_zero + numerical_impute_median:\n",
        "    TrainSet_imputed[col] = pd.to_numeric(TrainSet_imputed[col], downcast=\"integer\")\n",
        "    TestSet_imputed[col] = pd.to_numeric(TestSet_imputed[col], downcast=\"integer\")\n",
        "\n",
        "for col in categorical_fill_none:\n",
        "    TrainSet_imputed[col] = TrainSet_imputed[col].astype(\"object\")\n",
        "    TestSet_imputed[col] = TestSet_imputed[col].astype(\"object\")\n",
        "\n",
        "# Restore correct dtypes for passthrough columns\n",
        "for col in passthrough_cols:\n",
        "    orig_dtype = TrainSet[col].dtype\n",
        "    TrainSet_imputed[col] = TrainSet_imputed[col].astype(orig_dtype)\n",
        "    TestSet_imputed[col] = TestSet_imputed[col].astype(orig_dtype)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preview Results After Imputation Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TrainSet_imputed.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Check for remaining missing values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Remaining missing values in TrainSet:\")\n",
        "print(TrainSet_imputed.isnull().sum().sort_values(ascending=False).head())\n",
        "print(\"Remaining missing values in TestSet:\")\n",
        "print(TestSet_imputed.isnull().sum().sort_values(ascending=False).head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confirm new flags exist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nColumns added as missingness flags:\")\n",
        "print([col for col in TrainSet_imputed.columns if '_missing' in col])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert integer columns into int64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for df in [TrainSet_imputed, TestSet_imputed]:\n",
        "    int_columns = df.select_dtypes(include=['int8', 'int16', 'int32']).columns\n",
        "    df[int_columns] = df[int_columns].astype('int64')\n",
        "print(\"Conversion to int64 completed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post-Imputation Correlation & PPS Check\n",
        "\n",
        "Now that we've imputed missing values and added flags, we reassess the feature relationships.\n",
        "\n",
        "This helps us:\n",
        "- Re-confirm top predictors of `SalePrice`\n",
        "- Detect any new multicollinearity\n",
        "- Spot newly valuable features (e.g. missingness indicators)\n",
        "\n",
        "We'll examine both:\n",
        "- **Pearson/Spearman correlation** (for linear and monotonic relationships)\n",
        "- **Power Predictive Score (PPS)** (for general predictive strength)\n",
        "NOTE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Preview column data types\n",
        "print(\"Column data types after imputation:\")\n",
        "print(TrainSet_imputed.dtypes.value_counts())\n",
        "\n",
        "# 2. Get object columns for review\n",
        "object_cols = TrainSet_imputed.select_dtypes(include='object').columns.tolist()\n",
        "print(f\"\\nObject-type columns to review ({len(object_cols)}):\")\n",
        "print(object_cols)\n",
        "\n",
        "# 3. Get numeric columns for review\n",
        "numeric_cols = TrainSet_imputed.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "print(f\"\\nNumeric-type columns to review ({len(numeric_cols)}):\")\n",
        "print(numeric_cols)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we save the imputed datasets "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create folder if not exists\n",
        "os.makedirs(\"outputs/datasets/engineered\", exist_ok=True)\n",
        "\n",
        "# Save Train and Test Sets after imputation and feature transformations\n",
        "TrainSet_imputed.to_csv(\"outputs/datasets/engineered/TrainSet_Engineered.csv\", index=False)\n",
        "TestSet_imputed.to_csv(\"outputs/datasets/engineered/TestSet_Engineered.csv\", index=False)\n",
        "\n",
        "print(\"Engineered datasets saved successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluate Distribution Transformations\n",
        "\n",
        "Now we assess whether **distribution transformations** (e.g. log, Yeo-Johnson) could help normalize features and benefit certain algorithms (like linear models or KNN).\n",
        "\n",
        "We'll use a custom utility `FeatureEngineeringAnalysis` to preview the effect of various transformations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Custom Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import warnings\n",
        "from feature_engine import transformation as vt\n",
        "from feature_engine.outliers import Winsorizer\n",
        "from feature_engine.encoding import OrdinalEncoder\n",
        "sns.set(style=\"whitegrid\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def FeatureEngineeringAnalysis(df, analysis_type=None):\n",
        "    \"\"\"\n",
        "    - used for quick feature engineering on numerical and categorical variables\n",
        "    to decide which transformation can better transform the distribution shape\n",
        "    - Once transformed, use a reporting tool, like ydata-profiling, to evaluate distributions\n",
        "    \"\"\"\n",
        "    check_missing_values(df)\n",
        "    allowed_types = ['numerical', 'ordinal_encoder', 'outlier_winsorizer']\n",
        "    check_user_entry_on_analysis_type(analysis_type, allowed_types)\n",
        "    list_column_transformers = define_list_column_transformers(analysis_type)\n",
        "\n",
        "    # Loop in each variable and engineer the data according to the analysis type\n",
        "    df_feat_eng = pd.DataFrame([])\n",
        "    for column in df.columns:\n",
        "        # create additional columns (column_method) to apply the methods\n",
        "        df_feat_eng = pd.concat([df_feat_eng, df[column]], axis=1)\n",
        "        for method in list_column_transformers:\n",
        "            df_feat_eng[f\"{column}_{method}\"] = df[column]\n",
        "\n",
        "        # Apply transformers in respective column_transformers\n",
        "        df_feat_eng, list_applied_transformers = apply_transformers(\n",
        "            analysis_type, df_feat_eng, column)\n",
        "\n",
        "        # For each variable, assess how the transformations perform\n",
        "        transformer_evaluation(\n",
        "            column, list_applied_transformers, analysis_type, df_feat_eng)\n",
        "\n",
        "    return df_feat_eng\n",
        "\n",
        "\n",
        "def check_user_entry_on_analysis_type(analysis_type, allowed_types):\n",
        "    \"\"\" Check analysis type \"\"\"\n",
        "    if analysis_type is None:\n",
        "        raise SystemExit(\n",
        "            f\"You should pass analysis_type parameter as one of the following options: {allowed_types}\")\n",
        "    if analysis_type not in allowed_types:\n",
        "        raise SystemExit(\n",
        "            f\"analysis_type argument should be one of these options: {allowed_types}\")\n",
        "\n",
        "\n",
        "def check_missing_values(df):\n",
        "    if df.isna().sum().sum() != 0:\n",
        "        raise SystemExit(\n",
        "            f\"There is a missing value in your dataset. Please handle that before getting into feature engineering.\")\n",
        "\n",
        "\n",
        "def define_list_column_transformers(analysis_type):\n",
        "    \"\"\" Set suffix columns according to analysis_type\"\"\"\n",
        "    if analysis_type == 'numerical':\n",
        "        list_column_transformers = [\n",
        "            \"log_e\", \"log_10\", \"reciprocal\", \"power\", \"box_cox\", \"yeo_johnson\"]\n",
        "\n",
        "    elif analysis_type == 'ordinal_encoder':\n",
        "        list_column_transformers = [\"ordinal_encoder\"]\n",
        "\n",
        "    elif analysis_type == 'outlier_winsorizer':\n",
        "        list_column_transformers = ['iqr']\n",
        "\n",
        "    return list_column_transformers\n",
        "\n",
        "\n",
        "def apply_transformers(analysis_type, df_feat_eng, column):\n",
        "    for col in df_feat_eng.select_dtypes(include='category').columns:\n",
        "        df_feat_eng[col] = df_feat_eng[col].astype('object')\n",
        "\n",
        "    if analysis_type == 'numerical':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_Numerical(\n",
        "            df_feat_eng, column)\n",
        "\n",
        "    elif analysis_type == 'outlier_winsorizer':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_OutlierWinsorizer(\n",
        "            df_feat_eng, column)\n",
        "\n",
        "    elif analysis_type == 'ordinal_encoder':\n",
        "        df_feat_eng, list_applied_transformers = FeatEngineering_CategoricalEncoder(\n",
        "            df_feat_eng, column)\n",
        "\n",
        "    return df_feat_eng, list_applied_transformers\n",
        "\n",
        "\n",
        "def transformer_evaluation(column, list_applied_transformers, analysis_type, df_feat_eng):\n",
        "    # For each variable, assess how the transformations perform\n",
        "    print(f\"* Variable Analyzed: {column}\")\n",
        "    print(f\"* Applied transformation: {list_applied_transformers} \\n\")\n",
        "    for col in [column] + list_applied_transformers:\n",
        "\n",
        "        if analysis_type != 'ordinal_encoder':\n",
        "            DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "        else:\n",
        "            if col == column:\n",
        "                DiagnosticPlots_Categories(df_feat_eng, col)\n",
        "            else:\n",
        "                DiagnosticPlots_Numerical(df_feat_eng, col)\n",
        "\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Categories(df_feat_eng, col):\n",
        "    plt.figure(figsize=(4, 3))\n",
        "    sns.countplot(data=df_feat_eng, x=col, palette=[\n",
        "                  '#432371'], order=df_feat_eng[col].value_counts().index)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.suptitle(f\"{col}\", fontsize=30, y=1.05)\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "def DiagnosticPlots_Numerical(df, variable):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    sns.histplot(data=df, x=variable, kde=True, element=\"step\", ax=axes[0])\n",
        "    stats.probplot(df[variable], dist=\"norm\", plot=axes[1])\n",
        "    sns.boxplot(x=df[variable], ax=axes[2])\n",
        "\n",
        "    axes[0].set_title('Histogram')\n",
        "    axes[1].set_title('QQ Plot')\n",
        "    axes[2].set_title('Boxplot')\n",
        "    fig.suptitle(f\"{variable}\", fontsize=30, y=1.05)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def FeatEngineering_CategoricalEncoder(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "    try:\n",
        "        encoder = OrdinalEncoder(encoding_method='arbitrary', variables=[\n",
        "                                 f\"{column}_ordinal_encoder\"])\n",
        "        df_feat_eng = encoder.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_ordinal_encoder\")\n",
        "\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_ordinal_encoder\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked\n",
        "\n",
        "\n",
        "def FeatEngineering_OutlierWinsorizer(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "\n",
        "    # Winsorizer iqr\n",
        "    try:\n",
        "        disc = Winsorizer(\n",
        "            capping_method='iqr', tail='both', fold=1.5, variables=[f\"{column}_iqr\"])\n",
        "        df_feat_eng = disc.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_iqr\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_iqr\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked\n",
        "\n",
        "\n",
        "def FeatEngineering_Numerical(df_feat_eng, column):\n",
        "    list_methods_worked = []\n",
        "\n",
        "    # LogTransformer base e\n",
        "    try:\n",
        "        lt = vt.LogTransformer(variables=[f\"{column}_log_e\"])\n",
        "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_log_e\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_log_e\"], axis=1, inplace=True)\n",
        "\n",
        "    # LogTransformer base 10\n",
        "    try:\n",
        "        lt = vt.LogTransformer(variables=[f\"{column}_log_10\"], base='10')\n",
        "        df_feat_eng = lt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_log_10\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_log_10\"], axis=1, inplace=True)\n",
        "\n",
        "    # ReciprocalTransformer\n",
        "    try:\n",
        "        rt = vt.ReciprocalTransformer(variables=[f\"{column}_reciprocal\"])\n",
        "        df_feat_eng = rt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_reciprocal\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_reciprocal\"], axis=1, inplace=True)\n",
        "\n",
        "    # PowerTransformer\n",
        "    try:\n",
        "        pt = vt.PowerTransformer(variables=[f\"{column}_power\"])\n",
        "        df_feat_eng = pt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_power\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_power\"], axis=1, inplace=True)\n",
        "\n",
        "    # BoxCoxTransformer\n",
        "    try:\n",
        "        bct = vt.BoxCoxTransformer(variables=[f\"{column}_box_cox\"])\n",
        "        df_feat_eng = bct.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_box_cox\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_box_cox\"], axis=1, inplace=True)\n",
        "\n",
        "    # YeoJohnsonTransformer\n",
        "    try:\n",
        "        yjt = vt.YeoJohnsonTransformer(variables=[f\"{column}_yeo_johnson\"])\n",
        "        df_feat_eng = yjt.fit_transform(df_feat_eng)\n",
        "        list_methods_worked.append(f\"{column}_yeo_johnson\")\n",
        "    except Exception:\n",
        "        df_feat_eng.drop([f\"{column}_yeo_johnson\"], axis=1, inplace=True)\n",
        "\n",
        "    return df_feat_eng, list_methods_worked\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Categorical Encoding - Ordinal: replaces categories with ordinal numbers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 1: Select variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "categorical_features = ['BsmtExposure', 'BsmtFinType1', 'GarageFinish', 'KitchenQual']\n",
        "print(\"Categorical features:\", categorical_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 2: Create a separate DataFrame, with your variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = TrainSet_imputed[categorical_features].copy()\n",
        "df_engineering.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 3: Create engineered variables(s) by applying the transformation(s), assess engineered variables distribution and select the most suitable method for each variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pass only the original categorical features to avoid conflicts with already transformed columns\n",
        "df_engineering = FeatureEngineeringAnalysis(df=TrainSet_imputed[categorical_features], analysis_type='ordinal_encoder')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For all variables, the transformation is effective, since it converted categories to numbers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 4 - Apply the selected transformation to the Train and Test set\n",
        "    1. Create a transformer\n",
        "    2. Fit_transform into TrainSet\n",
        "    3. Transform into TestSet "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "encoder = OrdinalEncoder(encoding_method='arbitrary', variables = categorical_features)\n",
        "TrainSet_imputed = encoder.fit_transform(TrainSet_imputed)\n",
        "TestSet_imputed = encoder.transform(TestSet_imputed)\n",
        "\n",
        "print(\"* Categorical encoding - ordinal transformation done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Numerical Transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Step 1: Select Features to Analyze\n",
        "Since there many numerical features, we should prioritize:\n",
        "\n",
        "A. Features highly correlated with SalePrice\n",
        "From your correlation analysis, these are likely candidates:\n",
        "\n",
        "|  Variable   |  Why Analyze?                                               |\n",
        "|-------------|-------------------------------------------------------------|\n",
        "| GrLivArea   | Often skewed right, strong correlation                      |\n",
        "| GarageArea  | May benefit from log transformation                         |\n",
        "| TotalBsmtSF | Can vary widely and often right-skewed                      |\n",
        "|  1stFlrSF   | Similar to above                                            |\n",
        "| OverallQual | Ordinal, might not need transformation but worth visualizing|\n",
        "\n",
        "B. New or imputed variables\n",
        "\n",
        "|Variable    | Why Analyze?                            |\n",
        "|------------|-----------------------------------------|\n",
        "|LotFrontage | We imputed it — so let's check its shape|\n",
        "|MasVnrArea  | Many zeroes — investigate transformation|\n",
        "|YearBuilt   | Time-based but numeric                  |\n",
        "|OpenPorchSF | Could have long tail                    |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "top_numerical_features = [\n",
        "    'GrLivArea',\n",
        "    'GarageArea',\n",
        "    'TotalBsmtSF',\n",
        "    '1stFlrSF',\n",
        "    'LotFrontage',\n",
        "    'MasVnrArea',\n",
        "    'OpenPorchSF',\n",
        "    'YearBuilt',\n",
        "    'YearRemodAdd'\n",
        "]\n",
        "print(\"Top Numerical variables:\", top_numerical_features)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 2: Create a separate DataFrame, with your variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = TrainSet_imputed[top_numerical_features].copy()\n",
        "df_engineering.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 3: Create engineered variables(s) by applying the transformation(s), assess engineered variables distribution and select the most suitable method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = FeatureEngineeringAnalysis(df=df_engineering, analysis_type='numerical')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How We Chose Which Transformations to Apply\n",
        "\n",
        "Each numerical variable was evaluated across multiple transformation methods using three visual diagnostics:\n",
        "- **Histogram**: checked for symmetry and bell-shaped distribution\n",
        "- **QQ Plot**: checked alignment with the diagonal (indicating normality)\n",
        "- **Boxplot**: checked for outlier compression and spread\n",
        "\n",
        "We selected the transformation that yielded the best visual improvement while preserving interpretability. Below is a summary of the selected transformations:\n",
        "\n",
        "| Variable        | Transformation Applied | Reasoning                                                                 |\n",
        "|-----------------|------------------------|---------------------------------------------------------------------------|\n",
        "| `GrLivArea`     | `log_e`                | Reduced right-skew and improved normality visually                        |\n",
        "| `GarageArea`    | `yeo_johnson`          | Handles zero values, improved symmetry                                   |\n",
        "| `1stFlrSF`      | `log_10`               | Significantly improved QQ plot and histogram                             |\n",
        "| `TotalBsmtSF`   | `power`                | Strong visual improvement in distribution shape                          |\n",
        "| `LotFrontage`   | `yeo_johnson`          | Smoothed outliers and improved bell-shaped symmetry                      |\n",
        "| `OverallQual`   | None                   | Ordinal variable, distribution already clean and interpretable           |\n",
        "| `YearBuilt`     | None                   | Discrete time-based variable; transformations distorted interpretability |\n",
        "\n",
        "Transformations not selected (like `reciprocal` or `box_cox`) were either less interpretable or introduced new artifacts.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SmartCorrelatedSelection Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 1: Select variable(s): for this transformer, you don't need to select variables, since you need all variables for this transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 2: Create a separate DataFrame, with your variable(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_engineering = TrainSet_imputed.copy()\n",
        "df_engineering.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Step 3: Create engineered variables(s) applying the transformation(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from feature_engine.selection import SmartCorrelatedSelection\n",
        "corr_sel = SmartCorrelatedSelection(variables=None, method=\"spearman\", threshold=0.8, selection_method=\"variance\")\n",
        "\n",
        "corr_sel.fit_transform(df_engineering)\n",
        "corr_sel.correlated_feature_sets_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_sel.features_to_drop_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Push cleaned data to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can now push the changes to your GitHub repository, using the Git commands (git add, git commit, git push)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusions and Next Steps\n",
        "\n",
        "### Key Outcomes\n",
        "- All missing values were imputed using context-appropriate strategies.\n",
        "- A selection of transformations was done via visual diagnostics (histogram, QQ plot, boxplot). Several numerical features were temporarily transformed using techniques like `log`, `power`, and `yeo-johnson` and then analysed if they successful in reducing skewness and improving normality\n",
        "- SmartCorrelatedSelection indicated several features to be dropped with a threshold of 0.6\n",
        "\n",
        "    |Features to be dropped|\n",
        "    |------------|\n",
        "    |`2ndFlrSF`|\n",
        "    |`GarageYrBlt`|\n",
        "    |`1stFlrSF`|\n",
        "    |`GarageArea`|\n",
        "    |`OverallQual`|\n",
        "    |`YearBuilt`|\n",
        "    |`YearRemodAd`|\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Conduct feature scaling if required (depending on the chosen algorithm).\n",
        "- Train baseline models and compare performance (e.g., Linear Regression, Random Forest).\n",
        "- Apply feature importance analysis post-modeling to validate choices made during feature engineering."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
